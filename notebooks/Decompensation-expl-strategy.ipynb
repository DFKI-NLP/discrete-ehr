{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import utils\n",
    "import yaml\n",
    "from glob import glob\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataloader.data import MIMICDataset, get_tables, JointTabularFeature\n",
    "from dataloader.labels import get_labels\n",
    "from dataloader.utils import BinnedEvent, get_vocab\n",
    "from utils import prepare_batch, load_class, load_model, load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_config('28zyblao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['wandb_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params\n",
    "# params['min_word_count'] = 10000\n",
    "params['batch_size'] = 1\n",
    "# params['vocab_file'] = 'embeddings/sentences.mimic3.hourly.random.binned.train.counts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['joint_tables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_vocab = get_vocab(**params)\n",
    "\n",
    "tables = get_tables(vocab=joint_vocab,\n",
    "                    load=True,\n",
    "                    event_class=BinnedEvent,\n",
    "                    **params)\n",
    "\n",
    "labels = get_labels(DEVICE)\n",
    "\n",
    "val_set = MIMICDataset(datalist_file='val_listfile.csv', mode='TRAIN',\n",
    "                       tables=tables, labels=labels,\n",
    "                       limit=None,\n",
    "                       numericalize=True,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.load_model(params, joint_vocab, tables, DEVICE)\n",
    "loaded_epoch = re.findall(r'checkpoint_(\\d+)_', params['model_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=params['batch_size'],\n",
    "                                         collate_fn=partial(utils.min_batch,\n",
    "                                                            tables=tables,\n",
    "                                                            labels=labels,\n",
    "                                                            limit=720,\n",
    "                                                            event_limit=300),\n",
    "                                         shuffle=False, num_workers=0, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_embeddings = []\n",
    "timestep_embeddings = []\n",
    "filenames = []\n",
    "targets = []\n",
    "\n",
    "for batch in val_loader:\n",
    "    x, y_true, extra = prepare_batch(batch, DEVICE)\n",
    "    \n",
    "    if 1 not in y_true['decompensation'][0,1]:\n",
    "        continue\n",
    "\n",
    "    preds, outputs = model(*x.values())\n",
    "    output = {\"y_pred\": preds,\n",
    "              \"y_true\": y_true}\n",
    "    \n",
    "    patient_embeddings.append(outputs['patient'][0].detach().cpu().numpy())\n",
    "    timestep_embeddings.append(outputs['timesteps'][0].detach().cpu().numpy())\n",
    "    targets.append(np.concatenate([preds['decompensation'].detach().cpu().numpy(), \n",
    "                                  y_true['decompensation'][:,1].detach().cpu().numpy()], \n",
    "                                 0))\n",
    "    filenames.append(extra['filename'])\n",
    "    losses = {}\n",
    "#     for label in labels.values():\n",
    "#         losses[label.task] = label.loss(output)\n",
    "    if len(timestep_embeddings) == 1: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sections = 3\n",
    "d_section = timestep_embeddings[0].shape[1] // N_sections\n",
    "\n",
    "fig, axes = plt.subplots(1, N_sections+1, figsize=(8, 4), gridspec_kw={'width_ratios':[0.1, 1, 1, 1]})\n",
    "\n",
    "death = targets[0].T[:,[0]]\n",
    "death[targets[0].T[:,[1]] == 1] *= -1\n",
    "\n",
    "# rasterized for removing the lines in pdf export\n",
    "sns.heatmap(death, ax=axes[0], cmap='bwr', vmax=1, vmin=-1, cbar=False, rasterized=True)\n",
    "axes[0].set_yticks(range(24, len(death), 24))\n",
    "axes[0].set_yticklabels(range(1, len(death)//24))\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_title('$p_d$')\n",
    "\n",
    "for (i, table) in enumerate(['CHARTEVENTS', 'LABEVENTS', 'OUTPUTEVENTS']):\n",
    "    sns.heatmap(timestep_embeddings[0][:,i*d_section:(i+1)*d_section], \n",
    "                ax=axes[i+1], \n",
    "                cbar=False,\n",
    "                rasterized=True)\n",
    "    axes[i+1].set_yticks(range(24, len(death), 24))\n",
    "    axes[i+1].set_yticklabels([])\n",
    "    axes[i+1].set_xticks([])\n",
    "    axes[i+1].set_title(table)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('notebooks/decomp-timesteps.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients, TokenReferenceBase, visualization\n",
    "\n",
    "dataiterator = mimic_dataset(val_loader)\n",
    "\n",
    "TASK = 'decompensation'\n",
    "\n",
    "def to_batch(x, y_trues, extras, limit=None):\n",
    "    x = list(x.values())\n",
    "    batch =  Batch(\n",
    "        inputs=(x[0], x[1][:limit], x[2][:limit], x[3][:limit]),\n",
    "        labels=(y_trues[TASK],),\n",
    "        additional_args=(x[3])\n",
    "    )\n",
    "    return batch, (x, y_trues, extras)\n",
    "\n",
    "def task_forward(*inputs):\n",
    "    preds, _ = model.forward(*inputs)\n",
    "    return preds[TASK]\n",
    "\n",
    "ig = IntegratedGradients(task_forward)\n",
    "\n",
    "attribution_dfs = []\n",
    "def forward_with_sigmoid(inputs):\n",
    "    out = model(*inputs)\n",
    "    return out\n",
    "\n",
    "for i, (batch, (x, y_trues, extras)) in tqdm(enumerate(dataiterator)):\n",
    "    # skip masked\n",
    "    print(batch.labels[0])\n",
    "    if batch.labels[0][1] == 0.: continue\n",
    "        \n",
    "    model.zero_grad()\n",
    "    \n",
    "    inputs = tuple([batch.inputs[0]] + [input_text_transform(input) for input in batch.inputs[1:]])\n",
    "    out, insight = model(*inputs)\n",
    "    pat_repr = insight['patient'].detach().cpu()\n",
    "    pred = torch.sigmoid(out[TASK]).item()\n",
    "\n",
    "    baselines = [input*0. for input in batch.inputs]\n",
    "\n",
    "    try:\n",
    "        # generate reference for each sample\n",
    "        attr = ig.attribute(inputs=inputs,\n",
    "                            n_steps=1)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    print([a.shape for a in attr])\n",
    "    print([x.shape for x in batch.inputs])\n",
    "    if (pred > THRES) and batch.labels[0][0,1].item():\n",
    "        df = create_attribution_df(attr[1:], batch.inputs[1:])\n",
    "        attribution_dfs.append((extras['filename'], df))\n",
    "        if len(attribution_dfs) > 5: break\n",
    "    most_attributions.append(extract_most_attr(extras['filename'][0], batch, \n",
    "                                               attr, 10, pred, batch.labels[0][0,1].item()))\n",
    "    if 1 in y_trues['decompensation'][0,1]:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icu",
   "language": "python",
   "name": "icu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
